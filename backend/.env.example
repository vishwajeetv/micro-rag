# ============================================================================
# MICRO-RAG ENVIRONMENT CONFIGURATION
# ============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to git (it contains secrets!)
# ============================================================================


# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Environment: development, staging, or production
ENVIRONMENT=development

# Application name (used in logs, metrics)
APP_NAME=micro-rag

# API version prefix (e.g., /api/v1)
API_V1_PREFIX=/api

# Debug mode: true for development, false for production
DEBUG=true

# Host and port for the FastAPI server
HOST=0.0.0.0
PORT=8000


# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================

# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your-api-key-here

# Model for text generation (chat completions)
OPENAI_CHAT_MODEL=gpt-4

# Model for creating embeddings
# Options: text-embedding-3-small (cheapest), text-embedding-3-large (most accurate)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimension (1536 for text-embedding-3-small)
OPENAI_EMBEDDING_DIMENSION=1536

# Maximum tokens in LLM response
OPENAI_MAX_TOKENS=1000

# Temperature for response generation (0.0 = deterministic, 1.0 = creative)
OPENAI_TEMPERATURE=0.7


# ============================================================================
# POSTGRESQL + PGVECTOR CONFIGURATION
# ============================================================================

# PostgreSQL connection details
# Format: postgresql+asyncpg://user:password@host:port/database
DATABASE_URL=postgresql+asyncpg://microrag:microrag_password@localhost:5432/microrag

# Individual components (used by Docker and alembic)
POSTGRES_USER=microrag
POSTGRES_PASSWORD=microrag_password
POSTGRES_DB=microrag
POSTGRES_HOST=localhost
POSTGRES_PORT=5432

# Connection pool settings
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_TIMEOUT=30

# Enable SQL query logging (useful for debugging)
DB_ECHO=false

# Vector similarity search configuration
# Distance metric: cosine, l2 (euclidean), or inner_product
# cosine is recommended for text embeddings (normalizes for document length)
VECTOR_DISTANCE_METRIC=cosine

# HNSW Index Configuration (Hierarchical Navigable Small World)
# m: number of bidirectional links per node (4-64, default 16)
# Higher m = better recall but more memory. 16 is good for most cases
HNSW_M=16

# ef_construction: size of dynamic candidate list during index build (default 64)
# Higher = better index quality but slower build. 64-128 is good range
# For wiki with ~10k-50k chunks, 64 is fine
HNSW_EF_CONSTRUCTION=64

# ef_search: size of dynamic candidate list during search (default 40)
# Higher = better recall but slower queries. Can tune per query
# 40-100 is typical range. Start with 40 and increase if recall is low
HNSW_EF_SEARCH=40


# ============================================================================
# WEB SCRAPING CONFIGURATION
# ============================================================================

# Base URL for Europa Universalis 5 Wiki
EU5_WIKI_BASE_URL=https://eu5.paradoxwikis.com

# Starting page for scraping
EU5_WIKI_START_URL=https://eu5.paradoxwikis.com/Europa_Universalis_5_Wiki

# Maximum pages to scrape per job (safety limit)
SCRAPER_MAX_PAGES=500

# Rate limiting: delay between requests in seconds
SCRAPER_DELAY_SECONDS=1.0

# Concurrent requests limit
SCRAPER_CONCURRENCY=5

# Request timeout in seconds
SCRAPER_TIMEOUT_SECONDS=30

# Maximum retries for failed requests
SCRAPER_MAX_RETRIES=3

# User agent for scraping requests
SCRAPER_USER_AGENT=MicroRAG-Bot/1.0 (Educational Project; +https://github.com/yourusername/micro-rag)


# ============================================================================
# TEXT CHUNKING CONFIGURATION
# ============================================================================

# Chunk size in tokens (OpenAI tokens, not characters)
# Recommended: 800 tokens for good balance
CHUNK_SIZE_TOKENS=800

# Overlap between chunks in tokens
# Recommended: 200 tokens (25% overlap) to preserve context
CHUNK_OVERLAP_TOKENS=200

# Model for tokenization (should match embedding model)
CHUNK_TOKENIZER_MODEL=text-embedding-3-small


# ============================================================================
# RAG QUERY CONFIGURATION
# ============================================================================

# Number of document chunks to retrieve for each query
RAG_TOP_K=5

# Minimum similarity score threshold (0.0 to 1.0)
# Only return chunks with similarity >= this value
# For cosine: 0.7 is a good starting point
RAG_MIN_SCORE=0.7

# Include source citations in responses
RAG_INCLUDE_SOURCES=true

# Maximum context length to send to LLM (in tokens)
RAG_MAX_CONTEXT_TOKENS=3000


# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: json or console
# Use json for production (parseable by log aggregation tools)
# Use console for development (human-readable)
LOG_FORMAT=console

# Log output: stdout, file, or both
LOG_OUTPUT=stdout

# Log file path (only used if LOG_OUTPUT includes 'file')
LOG_FILE_PATH=logs/app.log

# Log rotation: max file size in MB before rotation
LOG_MAX_SIZE_MB=100

# Number of rotated log files to keep
LOG_BACKUP_COUNT=5


# ============================================================================
# CORS CONFIGURATION
# ============================================================================

# Allowed origins for CORS (comma-separated)
# Use * for development, specific domains for production
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Allow credentials (cookies, authorization headers)
CORS_ALLOW_CREDENTIALS=true

# Allowed HTTP methods
CORS_ALLOW_METHODS=GET,POST,PUT,DELETE,OPTIONS

# Allowed headers
CORS_ALLOW_HEADERS=*


# ============================================================================
# SECURITY & RATE LIMITING
# ============================================================================

# Enable API authentication
API_AUTH_ENABLED=false

# API key for authentication (generate a secure random string)
# Only used if API_AUTH_ENABLED=true
API_KEY=your-secure-api-key-here

# Rate limiting: max requests per minute per IP
RATE_LIMIT_PER_MINUTE=60

# Enable rate limiting
RATE_LIMIT_ENABLED=true


# ============================================================================
# MONITORING & OBSERVABILITY (Optional)
# ============================================================================

# Enable cost tracking for OpenAI API calls
ENABLE_COST_TRACKING=true

# Enable performance metrics collection
ENABLE_METRICS=true

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=https://your-sentry-dsn-here


# ============================================================================
# TESTING CONFIGURATION
# ============================================================================

# Override settings for tests
TEST_MODE=false

# Use mock services in tests (don't call real APIs)
USE_MOCK_SERVICES=true

# Test database URL (separate from development database)
TEST_DATABASE_URL=postgresql+asyncpg://microrag:microrag_password@localhost:5432/microrag_test